{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "ok同步成功\n",
    "本实验基于networkx和DeepSNAP库进行图数据的操作。DeepSNAP是一个Python库，可帮助在图形上进行高效的深度学习。DeepSNAP 支持灵活的图形操作，标准管道，异构图形和简单的 API。\n",
    "  本实验完成了一个可泛化的 GNN 堆叠模型，可以通过调整参数：model 来选择不同的GNN 层（包括 torch_geometric.nn 包中已集成的 GCN、GAT、GraphSage 层，以及我们在第七章中自定义的ourGNNLayer 层）。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0.dev20220920\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.version' has no attribute 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39m__version__)\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmps\u001B[49m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch.version' has no attribute 'mps'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpyg_nn\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Linear\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/netenv/lib/python3.10/site-packages/torch_geometric/__init__.py:4\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModuleType\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m import_module\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloader\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/netenv/lib/python3.10/site-packages/torch_geometric/data/__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Data\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhetero_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HeteroData\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtemporal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TemporalData\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/netenv/lib/python3.10/site-packages/torch_geometric/data/data.py:20\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tensor\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_sparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparseTensor\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_store\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     FeatureStore,\n\u001B[1;32m     24\u001B[0m     FeatureTensorType,\n\u001B[1;32m     25\u001B[0m     TensorAttr,\n\u001B[1;32m     26\u001B[0m     _field_status,\n\u001B[1;32m     27\u001B[0m )\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph_store\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     29\u001B[0m     EDGE_LAYOUT_TO_ATTR_NAME,\n\u001B[1;32m     30\u001B[0m     EdgeAttr,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     edge_tensor_type_to_adj_type,\n\u001B[1;32m     35\u001B[0m )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch_sparse'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePass_ing\n",
    "\n",
    "from deepsnap.graph import Graph\n",
    "from deepsnap.batch import Batch\n",
    "from deepsnap.dataset import GraphDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 数据介绍\n",
    "实验以 [Planetoid](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html?highlight=Planetoid#torch_geometric.datasets.Planetoid) 数据集中的 Cora 引文数据网络[1]为例进行节点分类任务。\n",
    "* Cora 是一个根据科学论文之间相互引用关系而构建的图数据集，其中包含了共 2708 篇科学出版物，即对应了图中的 2708 个节点，此外图中共有5429条连边。出版物根据其内容被分为7类（即节点的标签有7类），包括遗传算法、神经网络、强化学习等七个领域。\n",
    "* 连边表示文章间的引用关系，每篇文章都至少引用了一篇其它文章，或者被其它文章引用，没有任何一篇文章与其它文章完全没有联系。如果将文章视作图中的节点，则 Cora 数据集对应的是连通图。\n",
    "* 每篇文章都由一个1433维的词向量表示，因而图中每个样本点具有1433维特征。词向量的每个元素都对应一个具体单词，其有0和1两种取值，0表示该元素对应的词不在文章中，1则表示被包含在文章中。所有词都来源于一个具有1433个词的字典。\n",
    "* 在Cora数据集上，我们要判定不同文章所属的类别，即进行节点的多分类任务。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index fields: test_mask ignored.\n",
      "Index fields: val_mask ignored.\n",
      "Index fields: train_mask ignored.\n",
      "Cora: Graph(G=[], edge_index=[2, 10556], edge_label_index=[2, 10556], node_feature=[2708, 1433], node_label=[2708], node_label_index=[2708], task=[])\n",
      "There are 1624 training nodes\n",
      "There are 541 validation nodes\n",
      "There are 543 test nodes\n"
     ]
    }
   ],
   "source": [
    "# 读取Cora图数据\n",
    "from torch_geometric.datasets import Planetoid\n",
    "pyg_dataset = Planetoid('./planetoid', 'Cora')\n",
    "graphs = GraphDataset.pyg_to_graphs(pyg_dataset, verbose=True,\n",
    "                                            fixed_split=False, netlib=nx)\n",
    "dataset = GraphDataset(graphs, task='node')  # node, edge, link_pred, graph\n",
    "\n",
    "# 采用直推式学习（Transductive learning）进行数据集划分\n",
    "dataset_train, dataset_val, dataset_test \\\n",
    "    = dataset.split(transductive=True, split_ratio=[0.6, 0.2, 0.2])\n",
    "\n",
    "print(\"Cora: {}\".format(graphs[0]))\n",
    "print(\"There are {} training nodes\"\n",
    "      .format(dataset_train[0].node_label_index.shape[0]))\n",
    "print(\"There are {} validation nodes\"\n",
    "      .format(dataset_val[0].node_label_index.shape[0]))\n",
    "print(\"There are {} test nodes\"\n",
    "      .format(dataset_test[0].node_label_index.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 单层GNN模型\n",
    "消息传递范式因其简单且强大的特性，现被人们广泛地使用。基于此范式，我们可以定义聚合邻接节点信息来生成中心节点表征的图神经网络。在PyG中，MessagePassing 基类是所有基于消息传递范式的图神经网络的基类，它大大地方便了我们对图神经网络的构建。\n",
    "要自定义GNN模型，首先需要继承MessagePassing类，然后重写如下方法:\n",
    "* message(…)：构建要传递的消息；\n",
    "* aggregate(…)：将从源节点传递过来的消息聚合到目标节点；\n",
    "* update(…)：更新节点的消息。\n",
    "上述方法并不是一定都需要用户自定义。更多内容可以参考 [PyG官方文档](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html#the-messagepassing-base-class)。\n",
    "自定义单层GNN会首先调用forward()函数，forward()函数会调用propagate()函数，propagate()函数又会依次调用message(), aggregate(), update()。\n",
    "GraphSage可以选取不同的消息转换、聚合、更新函数，这里我们给出了一种简单的GNN层的实现，其对应公式如下：\n",
    "$$h_v^{(l)} = W_l \\cdot h_v^{(l-1)} + W_r \\cdot \\frac{1}{|N(v)| } \\sum_{u \\in N(v)} h_u^{(l-1)$$.\n",
    "代码中没有显示调用aagrgate() 和 update()函数，而在 forward() 中实现其功能。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ourGNNLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    自定义单层GNN模型\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, **kwargs):\n",
    "        super(ourGNNLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.lin_l = Linear(in_channels, out_channels)  # W_l，对中心节点应用\n",
    "        self.lin_r = Linear(in_channels, out_channels)  # W_r，对邻居节点应用\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, size=None):\n",
    "\n",
    "        # propagate() 函数中会依次调用message(), aggreage() 和update()函数\n",
    "        out = self.propagate(edge_index, x=(x, x), size=size)  #\n",
    "\n",
    "        x = self.lin_l(x)  # 自环\n",
    "        out = self.lin_r(out)  # 邻居信息\n",
    "        out = out + x\n",
    "        if self.normalize:  # L2正则化\n",
    "            out = F.normalize(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        return x_j\n",
    "\n",
    "\n",
    "    # def aggregate(self, inputs, index, dim_size=None):\n",
    "    #     out = torch_scatter.scatter(src=inputs, index = index, dim=self.node_dim, dim_size=dim_size, reduce='mean')\n",
    "    #     return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 多层GNN模型实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):     # 基于torch.nn.Module类，构建网络模型\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args):\n",
    "        \"\"\"\n",
    "        堆叠多层 GNN\n",
    "        :param input_dim: 输入维度\n",
    "        :param hidden_dim: 隐藏层维度\n",
    "        :param output_dim: 输出维度\n",
    "        :param args: 模型的其它参数\n",
    "        \"\"\"\n",
    "\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        # 选择GNN层的类型\n",
    "        conv_model = self.build_conv_model(args[\"model\"])\n",
    "        # 将多个GNN层堆叠起来\n",
    "        # 添加第一层\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        # 添加中间层\n",
    "        for l in range(args[\"num_layers\"] - 1):\n",
    "            self.convs.append(conv_model(hidden_dim, hidden_dim))\n",
    "        # 添加最后一层\n",
    "        self.convs.append(conv_model(hidden_dim,output_dim))\n",
    "        #self.post_mp = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "\n",
    "            # 激活函数，读者可以自行尝试不同的激活函数\n",
    "            x = F.leaky_relu(x)\n",
    "            # x = F.relu(x)\n",
    "\n",
    "            # droupout 操作\n",
    "            # dropout(input, p=0.5, training=True, inplace=False)\n",
    "            # 参数training默认为True，置True时应用Dropout，置False时不用。\n",
    "            # 因此在调用dropout()时，\n",
    "            # 将self.training传入函数，就可以在训练时应用dropout，评估时关闭dropout。\n",
    "            x = F.dropout(x, p= args[\"dropout\"], training=self.training)\n",
    "\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # the dim of x is [2708,7],since N=2708 and num_classes=7.\n",
    "        return x\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        \"\"\"\n",
    "        选择不同的GNN层\n",
    "        \"\"\"\n",
    "        if model_type == 'GCN':\n",
    "            return pyg_nn.GCNConv\n",
    "        elif model_type == 'ourGNNLayer':\n",
    "            return ourGNNLayer\n",
    "        elif model_type == 'GAT':\n",
    "            return pyg_nn.GATConv\n",
    "        elif model_type == \"GraphSage\":\n",
    "            return pyg_nn.SAGEConv\n",
    "        else:\n",
    "            raise ValueError(\"Model {} unavailable, \" \\\n",
    "                  \"please add it to GNN.build_conv_model.\".format(model_type))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 训练和测试过程\n",
    "\n",
    "（1）本实验基于Torch.autograd —— PyTorch 提供的自动求导的包，可以通过调整参数 opt 选择不同的优化方法（包括 adam, sgd, rmsprop, adagrad）。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args['weight_decay']\n",
    "    filter_fn = filter(lambda p: p.requires_grad, params)\n",
    "    if args['opt'] == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args['lr'], weight_decay=weight_decay)\n",
    "    elif args['opt'] == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args['lr'], momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args['opt'] == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args['lr'], weight_decay=weight_decay)\n",
    "    elif args['opt'] == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args['lr'], weight_decay=weight_decay)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（2）训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "def train(train_loader, val_loader, test_loader,\n",
    "          args, num_node_features, num_classes, device = \"cuda\"):\n",
    "\n",
    "    model = GNN(num_node_features, args['hidden_dim'], num_classes, args)\\\n",
    "        .to(device)\n",
    "\n",
    "    optimizer = build_optimizer(args, model.parameters())\n",
    "\n",
    "    train_acc, val_acc, test_acc = [], [], []\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 解码过程：直接把嵌入作为预测结果\n",
    "            pred = model(batch)\n",
    "            label = batch.node_label\n",
    "\n",
    "            loss = model.loss(pred[batch.node_label_index], label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"loss_grad:{}\".format(loss.grad))\n",
    "\n",
    "        # 记录每个epoch的结果\n",
    "        train_acc.append( test(train_loader, model, device) )\n",
    "        val_acc.append( test(val_loader, model, device) )\n",
    "        test_acc.append( test(test_loader, model, device) )\n",
    "        if epoch % 10 ==9:\n",
    "            print(\"Epoch {}: Train: {:.4f}, Validation: {:.4f}. Test: {:.4f}, Loss: {:.4f}\".format(\n",
    "                epoch + 1, train_acc[-1], val_acc[-1], test_acc[-1], total_loss))\n",
    "\n",
    "\n",
    "    return model,train_acc, val_acc, test_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（3）测试\n",
    "在pytorch中，tensor有一个名为requires_grad的参数，如果设置为True，则反向传播时，该tensor就会自动求导。\n",
    "tensor的requires_grad的属性默认为False，若一个节点（叶子变量：自己创建的tensor）requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True（即使其他相依赖的tensor的requires_grad = False） 。\n",
    "在pytorch搭建的网络中，torch.no_grad()非常常见。在该模块下，所有计算得出的tensor的requires_grad都自动设置为False，不自动计算梯度，也不会进行反向传播。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model, device='cuda'):\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        logits = model(batch)\n",
    "        pred = logits[batch.node_label_index].max(1)[1]\n",
    "        acc = pred.eq(batch.node_label).sum().item()\n",
    "        total = batch.node_label_index.shape[0]\n",
    "        acc /= total\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（4）设置参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "可调超参数\n",
    "\"\"\"\n",
    "args = {\n",
    "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"num_layers\": 2,              # 设置GNN层数\n",
    "    \"hidden_dim\" : 32,            # 隐藏层维数\n",
    "    \"epochs\" : 100,               # 循环次数\n",
    "    'dropout': 0.3,               # dropout概率\n",
    "    \"lr\" : 0.01,                  # 学习率\n",
    "    \"weight_decay\": 5e-4,         # 权重衰减项，防止过拟合的一个参数。\n",
    "                                  #   可以调节模型复杂度对损失函数的影响，\n",
    "                                  #   若weight decay很大，则复杂的模型损失函数的值也就大。\n",
    "    'opt': 'adam',                # 选择不同的优化函数，\n",
    "                                  #   可选参数为：'adam','sgd','rmsprop','adagrad'\n",
    "    \"model\" : 'ourGNNLayer',      # 选择不同的GNN层，\n",
    "                                  #   可选参数为：'GCN','GAT','GraphSage','ourGNNLayer'\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(), batch_size=1)\n",
    "val_loader   = DataLoader(dataset_val, collate_fn=Batch.collate(), batch_size=1)\n",
    "test_loader  = DataLoader(dataset_test, collate_fn=Batch.collate(), batch_size=1)\n",
    "\n",
    "num_node_features = dataset_train.num_node_features\n",
    "num_classes = dataset_train.num_node_labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（5）开始训练，并可视化训练结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9410, -2.1241, -1.5862,  ..., -2.6919, -1.7721, -1.9438],\n",
      "        [-2.0776, -1.7950, -1.5119,  ..., -2.7978, -1.8488, -1.9776],\n",
      "        [-2.4803, -2.5324, -1.4541,  ..., -2.0488, -1.9496, -2.0212],\n",
      "        ...,\n",
      "        [-2.4399, -1.9344, -1.7942,  ..., -2.4562, -1.7284, -1.8440],\n",
      "        [-2.4842, -2.2050, -1.7974,  ..., -1.8623, -1.9570, -2.4626],\n",
      "        [-2.5492, -1.7198, -1.9723,  ..., -2.0559, -1.9586, -2.3253]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_grad:None\n",
      "tensor([[-2.2758, -2.3241, -1.8883,  ..., -1.6870, -2.0592, -2.5336],\n",
      "        [-2.2165, -1.9910, -2.0551,  ..., -1.7859, -2.0407, -2.6554],\n",
      "        [-2.0512, -2.3538, -1.9506,  ..., -1.5969, -2.0168, -2.6571],\n",
      "        ...,\n",
      "        [-2.3305, -1.7737, -1.9139,  ..., -2.2709, -2.0297, -2.5147],\n",
      "        [-2.0854, -1.9338, -1.9810,  ..., -2.0476, -2.0151, -2.7250],\n",
      "        [-2.2173, -1.9807, -2.1307,  ..., -1.8094, -1.9820, -2.6505]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1708, -2.1959, -1.9500,  ..., -1.9844, -2.0946, -2.5282],\n",
      "        [-1.9172, -2.4931, -1.8931,  ..., -1.6419, -2.0495, -2.5506],\n",
      "        [-1.8830, -2.4589, -1.9775,  ..., -1.5780, -2.2117, -2.5285],\n",
      "        ...,\n",
      "        [-2.2050, -2.0005, -1.6360,  ..., -2.4460, -2.1674, -2.3650],\n",
      "        [-2.0975, -2.1159, -1.9055,  ..., -2.2198, -2.1979, -2.4572],\n",
      "        [-2.3048, -2.2784, -1.9966,  ..., -1.8559, -2.0834, -2.4423]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2510, -2.2016, -1.8563,  ..., -2.0504, -2.3870, -2.2707],\n",
      "        [-1.8217, -2.6311, -2.0937,  ..., -1.5849, -2.2191, -2.2343],\n",
      "        [-1.9379, -2.5603, -2.0100,  ..., -1.6020, -2.3333, -2.2618],\n",
      "        ...,\n",
      "        [-2.3864, -2.1130, -1.7265,  ..., -2.2394, -1.9371, -2.4446],\n",
      "        [-2.1671, -2.3557, -1.8432,  ..., -2.0424, -2.1864, -2.3507],\n",
      "        [-2.2381, -2.2189, -2.0628,  ..., -2.1469, -2.1458, -2.3049]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2482, -2.2478, -1.8134,  ..., -2.3003, -2.3210, -2.0752],\n",
      "        [-1.7947, -2.5501, -2.2217,  ..., -1.6025, -2.2132, -2.2486],\n",
      "        [-2.2940, -2.3308, -2.1238,  ..., -1.6397, -2.3339, -2.1777],\n",
      "        ...,\n",
      "        [-2.2200, -2.0359, -1.8566,  ..., -2.5709, -2.2029, -2.0263],\n",
      "        [-2.1931, -2.2360, -2.1023,  ..., -2.1355, -2.2104, -2.1178],\n",
      "        [-2.2533, -2.0361, -2.2505,  ..., -2.2240, -2.1891, -2.0998]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3735, -1.7701, -2.2067,  ..., -2.2779, -2.2788, -2.0552],\n",
      "        [-2.0655, -2.1679, -2.5670,  ..., -1.6640, -2.0141, -2.2544],\n",
      "        [-2.1185, -2.3257, -2.3699,  ..., -1.6472, -2.2148, -2.1618],\n",
      "        ...,\n",
      "        [-2.1351, -1.7773, -2.0660,  ..., -2.5963, -2.0254, -2.1082],\n",
      "        [-2.3226, -2.0113, -2.2293,  ..., -2.3350, -2.1025, -2.0709],\n",
      "        [-2.3371, -1.9809, -2.1988,  ..., -2.3411, -2.1651, -2.0393]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3288, -1.8451, -2.3366,  ..., -2.1078, -2.1772, -2.0344],\n",
      "        [-1.8603, -2.5336, -2.1906,  ..., -1.2684, -2.1972, -2.1324],\n",
      "        [-2.3188, -2.2830, -2.3233,  ..., -1.5538, -2.3018, -2.0332],\n",
      "        ...,\n",
      "        [-2.2921, -1.9063, -2.2937,  ..., -2.3779, -1.9683, -2.0885],\n",
      "        [-2.2231, -1.8013, -2.2064,  ..., -2.4907, -2.0883, -2.0146],\n",
      "        [-2.2545, -1.7207, -2.2357,  ..., -2.4322, -2.1213, -2.0786]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.4000, -1.8560, -2.1549,  ..., -2.1606, -2.4133, -1.8504],\n",
      "        [-1.9329, -2.4346, -2.2820,  ..., -1.2139, -2.2616, -2.1768],\n",
      "        [-2.0088, -2.3024, -2.2938,  ..., -1.3280, -2.4432, -2.1641],\n",
      "        ...,\n",
      "        [-2.2122, -2.4373, -1.9140,  ..., -2.3018, -1.4888, -1.7728],\n",
      "        [-2.3594, -1.8928, -2.3792,  ..., -1.7601, -2.3099, -2.1289],\n",
      "        [-2.4090, -1.7969, -2.2737,  ..., -2.0585, -2.2467, -2.0547]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3094, -2.0211, -2.4278,  ..., -1.5115, -2.3060, -2.1430],\n",
      "        [-1.9625, -2.4610, -2.2734,  ..., -1.1833, -2.1629, -2.1443],\n",
      "        [-2.0057, -2.3237, -2.3600,  ..., -1.3198, -2.3700, -2.1397],\n",
      "        ...,\n",
      "        [-2.2849, -1.8392, -2.5384,  ..., -2.2572, -1.8654, -2.0336],\n",
      "        [-2.1266, -1.8613, -2.3472,  ..., -2.2047, -2.2301, -2.1441],\n",
      "        [-2.1576, -1.6927, -2.3345,  ..., -2.1919, -2.2628, -2.2081]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2305, -1.8912, -2.3413,  ..., -1.9704, -2.3733, -2.1236],\n",
      "        [-1.9867, -2.2645, -2.3557,  ..., -1.2149, -2.3770, -2.1567],\n",
      "        [-1.7975, -2.2746, -2.4106,  ..., -1.3633, -2.3310, -2.2940],\n",
      "        ...,\n",
      "        [-2.1200, -1.8247, -2.0431,  ..., -2.6929, -2.0039, -1.9925],\n",
      "        [-2.0965, -2.0044, -2.3246,  ..., -2.1306, -2.2279, -2.1157],\n",
      "        [-2.1532, -2.0573, -2.3211,  ..., -2.0918, -2.3424, -2.0248]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 10: Train: 0.9002, Validation: 0.8706. Test: 0.8306, Loss: 1.3360\n",
      "tensor([[-2.2072, -2.0663, -2.3576,  ..., -2.0482, -2.2571, -2.0761],\n",
      "        [-2.2399, -2.1444, -2.4757,  ..., -1.3466, -2.2674, -2.1064],\n",
      "        [-1.9263, -2.3866, -2.3058,  ..., -1.4797, -2.4060, -2.1509],\n",
      "        ...,\n",
      "        [-2.4823, -2.2312, -1.9128,  ..., -1.6554, -2.4592, -1.9169],\n",
      "        [-2.2748, -2.0071, -2.2286,  ..., -2.2947, -2.1740, -1.9923],\n",
      "        [-2.2654, -1.9586, -2.2435,  ..., -2.2650, -2.1554, -2.0416]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1601, -1.9363, -2.1220,  ..., -2.3813, -2.2563, -2.1089],\n",
      "        [-1.8317, -2.2155, -2.2874,  ..., -1.3343, -2.4892, -2.2738],\n",
      "        [-2.0208, -2.4022, -2.0530,  ..., -1.6075, -2.4340, -2.2736],\n",
      "        ...,\n",
      "        [-2.1634, -2.1521, -1.9006,  ..., -2.3739, -2.2750, -2.0906],\n",
      "        [-1.8339, -2.1597, -2.2533,  ..., -2.3888, -2.2249, -2.1611],\n",
      "        [-1.9558, -2.2033, -2.1959,  ..., -2.3776, -2.2449, -2.0903]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.0817, -2.1814, -2.1955,  ..., -2.2772, -2.2678, -2.0486],\n",
      "        [-2.1778, -2.2786, -2.2698,  ..., -1.2863, -2.3797, -2.1583],\n",
      "        [-2.1522, -2.3501, -2.2199,  ..., -1.4469, -2.2878, -2.2808],\n",
      "        ...,\n",
      "        [-1.9153, -2.0552, -2.0886,  ..., -2.5183, -2.1563, -2.2191],\n",
      "        [-2.1547, -2.2021, -2.2115,  ..., -2.1757, -2.3106, -2.0689],\n",
      "        [-2.1021, -2.2705, -2.2021,  ..., -2.1874, -2.2600, -2.0965]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2513, -1.9813, -2.0177,  ..., -2.4392, -2.3016, -2.0238],\n",
      "        [-2.2103, -2.1491, -2.1008,  ..., -1.2328, -2.3598, -2.3837],\n",
      "        [-2.1265, -2.2272, -2.2431,  ..., -1.5015, -2.3507, -2.3325],\n",
      "        ...,\n",
      "        [-2.3066, -2.1920, -1.8443,  ..., -1.9330, -2.2856, -2.3700],\n",
      "        [-2.1133, -2.4178, -2.1353,  ..., -1.9851, -2.2789, -2.1086],\n",
      "        [-2.0600, -2.3332, -2.2705,  ..., -2.0790, -2.1776, -2.1378]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3563, -2.2031, -1.8969,  ..., -2.1538, -2.3292, -2.1053],\n",
      "        [-2.2224, -2.2194, -2.0330,  ..., -1.2147, -2.3106, -2.3927],\n",
      "        [-2.0994, -2.3224, -2.2164,  ..., -1.4247, -2.3630, -2.2845],\n",
      "        ...,\n",
      "        [-2.1747, -1.8889, -2.1566,  ..., -1.8348, -2.2596, -2.5616],\n",
      "        [-2.1099, -2.2317, -2.2470,  ..., -2.1404, -2.1948, -2.2095],\n",
      "        [-2.0428, -2.2617, -2.2448,  ..., -2.1756, -2.1584, -2.2376]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1817, -2.2570, -2.0863,  ..., -2.0311, -2.2800, -2.2702],\n",
      "        [-2.1147, -2.2241, -2.1992,  ..., -1.1352, -2.2273, -2.3208],\n",
      "        [-2.1563, -2.2696, -2.1130,  ..., -1.3994, -2.3905, -2.3193],\n",
      "        ...,\n",
      "        [-2.1813, -2.1632, -2.0276,  ..., -2.0679, -2.2393, -2.3608],\n",
      "        [-2.2384, -2.1155, -2.1947,  ..., -2.2122, -2.1754, -2.1884],\n",
      "        [-2.2977, -2.1662, -2.1571,  ..., -2.1878, -2.1628, -2.1453]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.0566, -2.1702, -2.1842,  ..., -2.2754, -2.1614, -2.2717],\n",
      "        [-1.9952, -2.3136, -2.2075,  ..., -1.1985, -1.9870, -2.4690],\n",
      "        [-2.2645, -2.2526, -2.1245,  ..., -1.8031, -2.2087, -2.3531],\n",
      "        ...,\n",
      "        [-2.2298, -2.3939, -1.9358,  ..., -2.0968, -2.1210, -2.2642],\n",
      "        [-2.1253, -2.3438, -2.1598,  ..., -2.0717, -2.0500, -2.3269],\n",
      "        [-2.1390, -2.2541, -2.1352,  ..., -2.3333, -2.1273, -2.1278]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3195, -2.2115, -2.0600,  ..., -2.1557, -2.2484, -2.1196],\n",
      "        [-2.0034, -2.1844, -2.2482,  ..., -1.1573, -2.1740, -2.4106],\n",
      "        [-1.9358, -2.2824, -2.2421,  ..., -1.5091, -2.4023, -2.3529],\n",
      "        ...,\n",
      "        [-1.9661, -2.0727, -1.8339,  ..., -2.2395, -2.2207, -2.5029],\n",
      "        [-2.1627, -2.2853, -2.3298,  ..., -1.9765, -2.1098, -2.2228],\n",
      "        [-2.2372, -2.2031, -2.3037,  ..., -1.9821, -2.1518, -2.2154]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.0912, -2.1297, -2.3382,  ..., -2.0655, -2.2758, -2.2027],\n",
      "        [-2.0237, -2.1557, -2.2913,  ..., -1.1402, -2.2948, -2.2576],\n",
      "        [-2.1880, -2.2862, -2.2188,  ..., -1.4762, -2.3202, -2.2932],\n",
      "        ...,\n",
      "        [-1.8232, -1.9829, -2.2463,  ..., -2.5001, -2.0323, -2.2820],\n",
      "        [-2.2139, -2.1767, -2.1960,  ..., -2.2259, -2.1994, -2.1220],\n",
      "        [-2.2175, -2.1836, -2.2347,  ..., -2.1646, -2.1706, -2.1691]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2772, -2.1074, -2.2061,  ..., -2.1186, -2.1785, -2.2347],\n",
      "        [-2.1475, -2.1909, -2.0646,  ..., -1.1486, -2.4120, -2.2116],\n",
      "        [-2.1650, -2.3028, -2.2142,  ..., -1.5788, -2.3798, -2.2188],\n",
      "        ...,\n",
      "        [-2.0645, -2.0437, -2.3186,  ..., -1.9868, -2.2273, -2.3860],\n",
      "        [-2.0965, -2.2972, -2.2377,  ..., -2.2531, -2.1523, -2.0805],\n",
      "        [-2.1040, -2.1770, -2.2411,  ..., -2.1955, -2.2542, -2.1635]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 20: Train: 0.9243, Validation: 0.9020. Test: 0.8600, Loss: 1.2293\n",
      "tensor([[-2.2153, -2.0301, -2.2834,  ..., -2.1681, -2.1957, -2.2119],\n",
      "        [-2.1024, -2.1566, -2.2504,  ..., -1.1354, -2.1599, -2.3118],\n",
      "        [-2.2892, -2.2676, -2.2445,  ..., -1.4677, -2.2868, -2.2263],\n",
      "        ...,\n",
      "        [-2.0202, -2.1794, -2.3954,  ..., -1.9318, -2.3315, -2.1705],\n",
      "        [-1.9968, -2.2350, -2.1338,  ..., -2.4049, -2.1864, -2.1104],\n",
      "        [-2.1483, -2.2862, -2.1378,  ..., -2.3069, -2.1942, -2.0357]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2845, -2.2083, -2.1070,  ..., -2.2223, -2.2218, -2.0655],\n",
      "        [-2.1788, -2.2730, -2.3122,  ..., -1.1487, -2.2324, -2.1456],\n",
      "        [-2.2622, -2.2937, -2.2237,  ..., -1.4438, -2.3314, -2.1967],\n",
      "        ...,\n",
      "        [-1.9992, -2.3195, -1.8881,  ..., -2.4419, -2.0551, -2.2574],\n",
      "        [-2.2293, -2.1596, -2.2170,  ..., -2.2078, -2.1229, -2.2027],\n",
      "        [-2.2373, -2.2284, -2.1750,  ..., -2.1897, -2.1223, -2.1866]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2583, -2.1228, -2.2362,  ..., -2.0899, -2.1972, -2.2197],\n",
      "        [-2.1108, -2.2381, -2.2910,  ..., -1.1394, -2.1560, -2.2326],\n",
      "        [-2.1717, -2.2568, -2.2757,  ..., -1.5726, -2.3127, -2.2778],\n",
      "        ...,\n",
      "        [-2.0763, -2.0558, -2.2287,  ..., -2.4262, -2.1328, -2.1107],\n",
      "        [-2.1411, -2.1288, -2.2889,  ..., -2.1207, -2.2276, -2.2225],\n",
      "        [-2.2255, -2.2221, -2.2226,  ..., -2.0842, -2.2277, -2.1487]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2151, -2.2199, -2.1213,  ..., -2.2216, -2.2290, -2.1305],\n",
      "        [-2.0901, -2.2006, -2.2397,  ..., -1.1346, -2.3432, -2.1808],\n",
      "        [-1.9967, -2.3570, -2.2545,  ..., -1.5122, -2.3952, -2.2378],\n",
      "        ...,\n",
      "        [-2.1655, -2.0422, -2.2095,  ..., -2.2207, -2.1528, -2.2797],\n",
      "        [-2.1478, -2.1874, -2.1918,  ..., -2.2060, -2.1797, -2.2310],\n",
      "        [-2.1664, -2.2010, -2.1296,  ..., -2.3207, -2.2207, -2.0808]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2733, -2.0208, -2.2344,  ..., -2.1807, -2.1565, -2.2370],\n",
      "        [-1.8642, -2.2585, -2.1704,  ..., -1.1750, -2.3442, -2.2773],\n",
      "        [-1.7684, -2.2825, -2.4333,  ..., -1.3291, -2.3511, -2.2271],\n",
      "        ...,\n",
      "        [-2.1800, -1.9804, -2.2306,  ..., -2.2276, -2.1265, -2.3106],\n",
      "        [-2.1756, -2.2761, -2.2201,  ..., -2.0008, -2.2380, -2.2021],\n",
      "        [-2.2469, -2.2406, -2.2322,  ..., -1.9479, -2.2372, -2.1917]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2421, -2.2526, -2.1813,  ..., -2.1267, -2.1785, -2.1561],\n",
      "        [-2.0828, -2.1536, -2.3262,  ..., -1.1274, -2.2201, -2.2148],\n",
      "        [-2.1973, -2.2880, -2.2705,  ..., -1.6655, -2.3010, -2.2082],\n",
      "        ...,\n",
      "        [-2.2219, -2.0722, -2.1641,  ..., -2.2224, -2.1698, -2.2439],\n",
      "        [-2.2362, -2.2022, -2.2037,  ..., -2.1448, -2.1811, -2.1750],\n",
      "        [-2.2638, -2.1863, -2.2138,  ..., -2.1418, -2.1343, -2.1953]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2042, -2.1703, -2.1036,  ..., -2.3239, -2.2221, -2.0958],\n",
      "        [-2.0221, -2.1479, -1.9694,  ..., -1.1649, -2.3450, -2.3166],\n",
      "        [-2.0226, -2.3537, -2.2452,  ..., -1.3581, -2.3061, -2.3270],\n",
      "        ...,\n",
      "        [-2.1833, -1.8745, -2.3976,  ..., -1.9866, -2.1977, -2.3541],\n",
      "        [-2.2013, -2.1319, -2.1182,  ..., -2.3053, -2.2015, -2.1698],\n",
      "        [-2.2283, -2.1455, -2.1272,  ..., -2.2809, -2.1646, -2.1880]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.3149, -2.2390, -2.1003,  ..., -2.0975, -2.1706, -2.1985],\n",
      "        [-2.0413, -2.1509, -2.2516,  ..., -1.1338, -2.1311, -2.3418],\n",
      "        [-1.8894, -2.1701, -2.3652,  ..., -1.5750, -2.2421, -2.4885],\n",
      "        ...,\n",
      "        [-1.9694, -2.2496, -2.3608,  ..., -1.8013, -2.1491, -2.4105],\n",
      "        [-2.1290, -2.2171, -2.1698,  ..., -2.2930, -2.1646, -2.1603],\n",
      "        [-2.0701, -2.1817, -2.1701,  ..., -2.3195, -2.2261, -2.1493]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2707, -2.2380, -2.1368,  ..., -2.1333, -2.1699, -2.1850],\n",
      "        [-2.2035, -2.2185, -2.2347,  ..., -1.1188, -2.2273, -2.1663],\n",
      "        [-2.1390, -2.2985, -2.3345,  ..., -1.3182, -2.2440, -2.2574],\n",
      "        ...,\n",
      "        [-2.2848, -2.1423, -2.2137,  ..., -1.9236, -2.2446, -2.2691],\n",
      "        [-2.0960, -2.1755, -2.1333,  ..., -2.3365, -2.1921, -2.1863],\n",
      "        [-2.1321, -2.2287, -2.1375,  ..., -2.3034, -2.2080, -2.1127]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2115, -2.2342, -2.1205,  ..., -2.2186, -2.1802, -2.1746],\n",
      "        [-2.2488, -2.1740, -2.2121,  ..., -1.1232, -2.1607, -2.2432],\n",
      "        [-1.9832, -2.1674, -2.3431,  ..., -1.1904, -2.2437, -2.3604],\n",
      "        ...,\n",
      "        [-2.3206, -2.1419, -2.2387,  ..., -1.9601, -2.1861, -2.2321],\n",
      "        [-2.1642, -2.2991, -2.1400,  ..., -2.1654, -2.1844, -2.1816],\n",
      "        [-2.1920, -2.3015, -2.1494,  ..., -2.1379, -2.1838, -2.1690]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 30: Train: 0.9421, Validation: 0.8909. Test: 0.8637, Loss: 1.1978\n",
      "tensor([[-2.3126, -2.2692, -2.0645,  ..., -2.0936, -2.1947, -2.1697],\n",
      "        [-2.1038, -2.3129, -2.2040,  ..., -1.1292, -2.1663, -2.2251],\n",
      "        [-2.1132, -2.3106, -2.2509,  ..., -1.4065, -2.2805, -2.3139],\n",
      "        ...,\n",
      "        [-2.1853, -2.1064, -2.3330,  ..., -1.9763, -2.3044, -2.1785],\n",
      "        [-2.2006, -2.1769, -2.1905,  ..., -2.1714, -2.1730, -2.2320],\n",
      "        [-2.1340, -2.2078, -2.2045,  ..., -2.2306, -2.1229, -2.2365]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2616, -2.2692, -2.0480,  ..., -2.1896, -2.1537, -2.1997],\n",
      "        [-2.1394, -2.2802, -2.1118,  ..., -1.1291, -2.1996, -2.2924],\n",
      "        [-2.0118, -2.4148, -2.2130,  ..., -1.3848, -2.3118, -2.2894],\n",
      "        ...,\n",
      "        [-2.2073, -1.8531, -2.3507,  ..., -2.2938, -2.1697, -2.1562],\n",
      "        [-2.1251, -2.2029, -2.1882,  ..., -2.2369, -2.1515, -2.2300],\n",
      "        [-2.1713, -2.2144, -2.2086,  ..., -2.1441, -2.1859, -2.2187]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1875, -2.2570, -2.1368,  ..., -2.1905, -2.1721, -2.1971],\n",
      "        [-2.3005, -2.1942, -2.2702,  ..., -1.1303, -2.1431, -2.1660],\n",
      "        [-2.1936, -2.3790, -2.2379,  ..., -1.3863, -2.1801, -2.3042],\n",
      "        ...,\n",
      "        [-2.2695, -2.3843, -2.1994,  ..., -1.8337, -2.1602, -2.1778],\n",
      "        [-2.1658, -2.1260, -2.2120,  ..., -2.1800, -2.1400, -2.3063],\n",
      "        [-2.2048, -2.1592, -2.2574,  ..., -2.0792, -2.1390, -2.2759]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2116, -2.2304, -2.1304,  ..., -2.1832, -2.1810, -2.2056],\n",
      "        [-2.1175, -2.3036, -2.3624,  ..., -1.1557, -2.1515, -2.1795],\n",
      "        [-2.1730, -2.4039, -2.2684,  ..., -1.4468, -2.1858, -2.2610],\n",
      "        ...,\n",
      "        [-2.1728, -2.1954, -2.1802,  ..., -2.1917, -2.0954, -2.2889],\n",
      "        [-2.0624, -2.2169, -2.1235,  ..., -2.2920, -2.2180, -2.2011],\n",
      "        [-2.1000, -2.2057, -2.1525,  ..., -2.2910, -2.1799, -2.2002]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2161, -2.2193, -2.1485,  ..., -2.1901, -2.1691, -2.1959],\n",
      "        [-2.0225, -2.2653, -2.2357,  ..., -1.1340, -2.1879, -2.2879],\n",
      "        [-2.0113, -2.3243, -2.3254,  ..., -1.2910, -2.2245, -2.3449],\n",
      "        ...,\n",
      "        [-2.2780, -2.0501, -2.1452,  ..., -2.2622, -2.1683, -2.2049],\n",
      "        [-2.1692, -2.2739, -2.2257,  ..., -2.0728, -2.1177, -2.2592],\n",
      "        [-2.1757, -2.2067, -2.2378,  ..., -2.0981, -2.1678, -2.2465]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2689, -2.1814, -2.1300,  ..., -2.1821, -2.1772, -2.1985],\n",
      "        [-2.3232, -2.2274, -2.0634,  ..., -1.1376, -2.1455, -2.2817],\n",
      "        [-2.3844, -2.2292, -2.1855,  ..., -1.3443, -2.1889, -2.2972],\n",
      "        ...,\n",
      "        [-2.1313, -2.0076, -2.3002,  ..., -2.3014, -2.0813, -2.2164],\n",
      "        [-2.1225, -2.1356, -2.1757,  ..., -2.2896, -2.1626, -2.2438],\n",
      "        [-2.1967, -2.2072, -2.1736,  ..., -2.1745, -2.1339, -2.2543]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2606, -2.2236, -2.1684,  ..., -2.0623, -2.1977, -2.2156],\n",
      "        [-2.1584, -2.2476, -2.2035,  ..., -1.1144, -2.1714, -2.2170],\n",
      "        [-2.1535, -2.3348, -2.2287,  ..., -1.5278, -2.3040, -2.2752],\n",
      "        ...,\n",
      "        [-2.1697, -2.1774, -2.2303,  ..., -2.1493, -2.1438, -2.2310],\n",
      "        [-2.1229, -2.2347, -2.1756,  ..., -2.2592, -2.1594, -2.1856],\n",
      "        [-2.1768, -2.2588, -2.1223,  ..., -2.1923, -2.2052, -2.1782]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2316, -2.2053, -2.1268,  ..., -2.2442, -2.1444, -2.1860],\n",
      "        [-2.3499, -2.2278, -2.2808,  ..., -1.1539, -2.1619, -2.0879],\n",
      "        [-2.1703, -2.3446, -2.2506,  ..., -1.2134, -2.1953, -2.2767],\n",
      "        ...,\n",
      "        [-2.2796, -2.1810, -2.1462,  ..., -2.1487, -2.1691, -2.1907],\n",
      "        [-2.1481, -2.2590, -2.1638,  ..., -2.1716, -2.2069, -2.1908],\n",
      "        [-2.1654, -2.2961, -2.1394,  ..., -2.1362, -2.2141, -2.1790]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2173, -2.1459, -2.1500,  ..., -2.2246, -2.2092, -2.1945],\n",
      "        [-1.8456, -2.2293, -2.0718,  ..., -1.1908, -2.2303, -2.3940],\n",
      "        [-2.0349, -2.3229, -2.1955,  ..., -1.3716, -2.2602, -2.4316],\n",
      "        ...,\n",
      "        [-2.2395, -2.1600, -2.1495,  ..., -2.2327, -2.2227, -2.1329],\n",
      "        [-2.1901, -2.2467, -2.1580,  ..., -2.2032, -2.1529, -2.1912],\n",
      "        [-2.1933, -2.2594, -2.1151,  ..., -2.1846, -2.1926, -2.1909]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1941, -2.2172, -2.1267,  ..., -2.2185, -2.1822, -2.2023],\n",
      "        [-2.2447, -2.2217, -2.2047,  ..., -1.1166, -2.2038, -2.1402],\n",
      "        [-2.2428, -2.2825, -2.2279,  ..., -1.2174, -2.2923, -2.2047],\n",
      "        ...,\n",
      "        [-2.2518, -2.1948, -2.1061,  ..., -2.2484, -2.1411, -2.1852],\n",
      "        [-2.0737, -2.2305, -2.1933,  ..., -2.2989, -2.1529, -2.1764],\n",
      "        [-2.0845, -2.1543, -2.1933,  ..., -2.2911, -2.2119, -2.1925]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 40: Train: 0.9532, Validation: 0.8928. Test: 0.8637, Loss: 1.1816\n",
      "tensor([[-2.1846, -2.2261, -2.1685,  ..., -2.1968, -2.1680, -2.2002],\n",
      "        [-2.2210, -2.1755, -2.1822,  ..., -1.1152, -2.2379, -2.1449],\n",
      "        [-2.1700, -2.3002, -2.1964,  ..., -1.2063, -2.3064, -2.2467],\n",
      "        ...,\n",
      "        [-2.1756, -2.0741, -2.2658,  ..., -2.2979, -2.1820, -2.1045],\n",
      "        [-2.1422, -2.1829, -2.1940,  ..., -2.2534, -2.1708, -2.1981],\n",
      "        [-2.1428, -2.1795, -2.1899,  ..., -2.2413, -2.1824, -2.2067]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1482, -2.1189, -2.1676,  ..., -2.2748, -2.1980, -2.2242],\n",
      "        [-2.3393, -2.1930, -2.2362,  ..., -1.1363, -2.1335, -2.1888],\n",
      "        [-2.2598, -2.3180, -2.2805,  ..., -1.2347, -2.1517, -2.2443],\n",
      "        ...,\n",
      "        [-2.2275, -2.1090, -2.1995,  ..., -2.2417, -2.1897, -2.1697],\n",
      "        [-2.1512, -2.1476, -2.2311,  ..., -2.2260, -2.1513, -2.2297],\n",
      "        [-2.1711, -2.2095, -2.1942,  ..., -2.1964, -2.1648, -2.2085]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2374, -2.1874, -2.1707,  ..., -2.1321, -2.2243, -2.1830],\n",
      "        [-2.0028, -2.2675, -2.1726,  ..., -1.1306, -2.2548, -2.2083],\n",
      "        [-1.9771, -2.2676, -2.3098,  ..., -1.2335, -2.2544, -2.3669],\n",
      "        ...,\n",
      "        [-2.1831, -2.1725, -2.2021,  ..., -2.3123, -2.1474, -2.1119],\n",
      "        [-2.1756, -2.2571, -2.1526,  ..., -2.2111, -2.1768, -2.1680],\n",
      "        [-2.1848, -2.2297, -2.1669,  ..., -2.1863, -2.2005, -2.1759]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1794, -2.1628, -2.2069,  ..., -2.2196, -2.2028, -2.1687],\n",
      "        [-2.3433, -2.1721, -2.1396,  ..., -1.1239, -2.1656, -2.1690],\n",
      "        [-2.1436, -2.2970, -2.3347,  ..., -1.2603, -2.2337, -2.2570],\n",
      "        ...,\n",
      "        [-2.1667, -2.1049, -2.2065,  ..., -2.3214, -2.1545, -2.1652],\n",
      "        [-2.2335, -2.1938, -2.1932,  ..., -2.1422, -2.1604, -2.2192],\n",
      "        [-2.2125, -2.2083, -2.1996,  ..., -2.1555, -2.1808, -2.1881]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2160, -2.1367, -2.2285,  ..., -2.1268, -2.1950, -2.2338],\n",
      "        [-2.1094, -2.2456, -2.3154,  ..., -1.1250, -2.1800, -2.1588],\n",
      "        [-2.0915, -2.2956, -2.2938,  ..., -1.1568, -2.2390, -2.2209],\n",
      "        ...,\n",
      "        [-2.1675, -2.1254, -2.2129,  ..., -2.2948, -2.1725, -2.1548],\n",
      "        [-2.0837, -2.1929, -2.1777,  ..., -2.3066, -2.1798, -2.1866],\n",
      "        [-2.0784, -2.1554, -2.2162,  ..., -2.3004, -2.1927, -2.1819]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2462, -2.1777, -2.1653,  ..., -2.1516, -2.2139, -2.1869],\n",
      "        [-2.0279, -2.2131, -2.2315,  ..., -1.1248, -2.1963, -2.2224],\n",
      "        [-1.7964, -2.3892, -2.2227,  ..., -1.3874, -2.3511, -2.3586],\n",
      "        ...,\n",
      "        [-2.1935, -1.9485, -2.2647,  ..., -2.3059, -2.2256, -2.1473],\n",
      "        [-2.1739, -2.2006, -2.1873,  ..., -2.2036, -2.1766, -2.2038],\n",
      "        [-2.1488, -2.1997, -2.1727,  ..., -2.2319, -2.1901, -2.2002]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2137, -2.2307, -2.1380,  ..., -2.1819, -2.2235, -2.1521],\n",
      "        [-2.1682, -2.2473, -2.2184,  ..., -1.1143, -2.1493, -2.1898],\n",
      "        [-2.0655, -2.3528, -2.2831,  ..., -1.2809, -2.2621, -2.2899],\n",
      "        ...,\n",
      "        [-2.1721, -2.1398, -2.1822,  ..., -2.3340, -2.1876, -2.1079],\n",
      "        [-2.2207, -2.2436, -2.1269,  ..., -2.1768, -2.2137, -2.1545],\n",
      "        [-2.2246, -2.2849, -2.1341,  ..., -2.1637, -2.1884, -2.1367]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1698, -2.1812, -2.1812,  ..., -2.2264, -2.2227, -2.1603],\n",
      "        [-1.9420, -2.1928, -2.0309,  ..., -1.1746, -2.2238, -2.2246],\n",
      "        [-2.0210, -2.3448, -2.1612,  ..., -1.1816, -2.2512, -2.3426],\n",
      "        ...,\n",
      "        [-2.2854, -2.2219, -2.1760,  ..., -2.1585, -2.1809, -2.1046],\n",
      "        [-2.1501, -2.1838, -2.2118,  ..., -2.1807, -2.1821, -2.2347],\n",
      "        [-2.1745, -2.2161, -2.1976,  ..., -2.1651, -2.1825, -2.2093]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2207, -2.1759, -2.1683,  ..., -2.1864, -2.1936, -2.2003],\n",
      "        [-1.9800, -2.2565, -2.2333,  ..., -1.1342, -2.2107, -2.1873],\n",
      "        [-2.1022, -2.2914, -2.2358,  ..., -1.2054, -2.2411, -2.3439],\n",
      "        ...,\n",
      "        [-2.2324, -2.1868, -2.1816,  ..., -2.2120, -2.1884, -2.1413],\n",
      "        [-2.1492, -2.2183, -2.1864,  ..., -2.2185, -2.1776, -2.1940],\n",
      "        [-2.1793, -2.2232, -2.1794,  ..., -2.1914, -2.1957, -2.1759]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1554, -2.2363, -2.1412,  ..., -2.2347, -2.2065, -2.1661],\n",
      "        [-2.1409, -2.1904, -2.0979,  ..., -1.1201, -2.1770, -2.2524],\n",
      "        [-2.1832, -2.2283, -2.1306,  ..., -1.1780, -2.2721, -2.3658],\n",
      "        ...,\n",
      "        [-2.1544, -2.1768, -2.1953,  ..., -2.3027, -2.1682, -2.1356],\n",
      "        [-2.1767, -2.2139, -2.1900,  ..., -2.1744, -2.1896, -2.2010],\n",
      "        [-2.1549, -2.2003, -2.1915,  ..., -2.2101, -2.1991, -2.1894]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 50: Train: 0.9594, Validation: 0.8891. Test: 0.8582, Loss: 1.1696\n",
      "tensor([[-2.1771, -2.1889, -2.1736,  ..., -2.2372, -2.1793, -2.1849],\n",
      "        [-2.1525, -2.1774, -2.3112,  ..., -1.1231, -2.2110, -2.0831],\n",
      "        [-1.9884, -2.3033, -2.3662,  ..., -1.1996, -2.2482, -2.2393],\n",
      "        ...,\n",
      "        [-2.1979, -2.2703, -2.2011,  ..., -2.1544, -2.1646, -2.1501],\n",
      "        [-2.1544, -2.1410, -2.2398,  ..., -2.2114, -2.1757, -2.2174],\n",
      "        [-2.1630, -2.1930, -2.2174,  ..., -2.1927, -2.1815, -2.1970]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2219, -2.1963, -2.1835,  ..., -2.2002, -2.1692, -2.1730],\n",
      "        [-2.1941, -2.1673, -2.2294,  ..., -1.1144, -2.2058, -2.1464],\n",
      "        [-1.9401, -2.3193, -2.2204,  ..., -1.2071, -2.2863, -2.3476],\n",
      "        ...,\n",
      "        [-2.2593, -2.0740, -2.2185,  ..., -2.1969, -2.2031, -2.1791],\n",
      "        [-2.2048, -2.2509, -2.1595,  ..., -2.1887, -2.1593, -2.1782],\n",
      "        [-2.2215, -2.2471, -2.1646,  ..., -2.1675, -2.1729, -2.1684]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1839, -2.2323, -2.1724,  ..., -2.1851, -2.1946, -2.1721],\n",
      "        [-2.0367, -2.2627, -2.1708,  ..., -1.1272, -2.1959, -2.1954],\n",
      "        [-2.1684, -2.2920, -2.3362,  ..., -1.1779, -2.1655, -2.2198],\n",
      "        ...,\n",
      "        [-2.2644, -2.0626, -2.3593,  ..., -2.1175, -2.1668, -2.1229],\n",
      "        [-2.1443, -2.2089, -2.2092,  ..., -2.2172, -2.1716, -2.1917],\n",
      "        [-2.1564, -2.2177, -2.1879,  ..., -2.2130, -2.1892, -2.1804]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2225, -2.2214, -2.1685,  ..., -2.1726, -2.1727, -2.1860],\n",
      "        [-2.1815, -2.1934, -2.0776,  ..., -1.1272, -2.1983, -2.1316],\n",
      "        [-2.1459, -2.2531, -2.2512,  ..., -1.1563, -2.2512, -2.2720],\n",
      "        ...,\n",
      "        [-2.2022, -2.2007, -2.1907,  ..., -2.2051, -2.1703, -2.1758],\n",
      "        [-2.1469, -2.1788, -2.2276,  ..., -2.1958, -2.1790, -2.2154],\n",
      "        [-2.1811, -2.2082, -2.2123,  ..., -2.1698, -2.1793, -2.1948]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1684, -2.1849, -2.1801,  ..., -2.2514, -2.1790, -2.1784],\n",
      "        [-2.0808, -2.1198, -2.2135,  ..., -1.1218, -2.2262, -2.2243],\n",
      "        [-2.1405, -2.2521, -2.1786,  ..., -1.1287, -2.2397, -2.2717],\n",
      "        ...,\n",
      "        [-2.2205, -2.1517, -2.1667,  ..., -2.1930, -2.2115, -2.1987],\n",
      "        [-2.1876, -2.2071, -2.2040,  ..., -2.1765, -2.1833, -2.1872],\n",
      "        [-2.1803, -2.2170, -2.2022,  ..., -2.1791, -2.1833, -2.1837]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1941, -2.2165, -2.1819,  ..., -2.1958, -2.1837, -2.1738],\n",
      "        [-2.2842, -2.1444, -2.0921,  ..., -1.1203, -2.2004, -2.1871],\n",
      "        [-2.2266, -2.2252, -2.2289,  ..., -1.1266, -2.2437, -2.1748],\n",
      "        ...,\n",
      "        [-2.2266, -2.1132, -2.2268,  ..., -2.2056, -2.0789, -2.2723],\n",
      "        [-2.1660, -2.2203, -2.1828,  ..., -2.2205, -2.1666, -2.1876],\n",
      "        [-2.1759, -2.2099, -2.1823,  ..., -2.1996, -2.1918, -2.1867]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2049, -2.2384, -2.1512,  ..., -2.2061, -2.1695, -2.1715],\n",
      "        [-2.2779, -2.2058, -2.1619,  ..., -1.1182, -2.1723, -2.2026],\n",
      "        [-2.0097, -2.2059, -2.2586,  ..., -1.1668, -2.3081, -2.3220],\n",
      "        ...,\n",
      "        [-1.9810, -2.0706, -2.3200,  ..., -2.2947, -2.2235, -2.1877],\n",
      "        [-2.1406, -2.1590, -2.2226,  ..., -2.2359, -2.1683, -2.2127],\n",
      "        [-2.1287, -2.1221, -2.2389,  ..., -2.2367, -2.1970, -2.2126]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2039, -2.1641, -2.2022,  ..., -2.1986, -2.1615, -2.2113],\n",
      "        [-2.1802, -2.2418, -2.1523,  ..., -1.1160, -2.1630, -2.1441],\n",
      "        [-2.2074, -2.2484, -2.3446,  ..., -1.1712, -2.2358, -2.1458],\n",
      "        ...,\n",
      "        [-2.1520, -2.1920, -2.2148,  ..., -2.2104, -2.1983, -2.1771],\n",
      "        [-2.1670, -2.1906, -2.1971,  ..., -2.2117, -2.1779, -2.2001],\n",
      "        [-2.1573, -2.1703, -2.2001,  ..., -2.2177, -2.2033, -2.1960]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2173, -2.2108, -2.1567,  ..., -2.1834, -2.1940, -2.1826],\n",
      "        [-2.1394, -2.2013, -2.2152,  ..., -1.1130, -2.1972, -2.1816],\n",
      "        [-2.1156, -2.2939, -2.2643,  ..., -1.1696, -2.2113, -2.2946],\n",
      "        ...,\n",
      "        [-2.2081, -2.2324, -2.1805,  ..., -2.2610, -2.1708, -2.0760],\n",
      "        [-2.1740, -2.1577, -2.2124,  ..., -2.2040, -2.1948, -2.2012],\n",
      "        [-2.1913, -2.1985, -2.2003,  ..., -2.1810, -2.1763, -2.1975]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1979, -2.2210, -2.1562,  ..., -2.2004, -2.1933, -2.1759],\n",
      "        [-2.1621, -2.1790, -2.0790,  ..., -1.1218, -2.2037, -2.2048],\n",
      "        [-2.1511, -2.2664, -2.1543,  ..., -1.1380, -2.2481, -2.2927],\n",
      "        ...,\n",
      "        [-2.2361, -2.2214, -2.1660,  ..., -2.1891, -2.1821, -2.1457],\n",
      "        [-2.1709, -2.1492, -2.2245,  ..., -2.2018, -2.1726, -2.2200],\n",
      "        [-2.1667, -2.1585, -2.2123,  ..., -2.2055, -2.1851, -2.2158]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 60: Train: 0.9717, Validation: 0.8891. Test: 0.8545, Loss: 1.1610\n",
      "tensor([[-2.1963, -2.2017, -2.1743,  ..., -2.2029, -2.1921, -2.1784],\n",
      "        [-2.2138, -2.1956, -2.1187,  ..., -1.1147, -2.1702, -2.1997],\n",
      "        [-2.0708, -2.2644, -2.2328,  ..., -1.1863, -2.2474, -2.3618],\n",
      "        ...,\n",
      "        [-2.2221, -2.2711, -2.1034,  ..., -2.2217, -2.2337, -2.0618],\n",
      "        [-2.1499, -2.2061, -2.1871,  ..., -2.2291, -2.1803, -2.1904],\n",
      "        [-2.1410, -2.1724, -2.1915,  ..., -2.2348, -2.2102, -2.1928]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2145, -2.2285, -2.1665,  ..., -2.1756, -2.1862, -2.1727],\n",
      "        [-2.1378, -2.2088, -2.2442,  ..., -1.1159, -2.1603, -2.2420],\n",
      "        [-2.0469, -2.2546, -2.2516,  ..., -1.1626, -2.2126, -2.3559],\n",
      "        ...,\n",
      "        [-2.2359, -2.1986, -2.1911,  ..., -2.1751, -2.1795, -2.1629],\n",
      "        [-2.1735, -2.1850, -2.2051,  ..., -2.2198, -2.1692, -2.1913],\n",
      "        [-2.1872, -2.1800, -2.1893,  ..., -2.2066, -2.1979, -2.1852]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2030, -2.1823, -2.1903,  ..., -2.2012, -2.1902, -2.1787],\n",
      "        [-2.3183, -2.1788, -2.1521,  ..., -1.1204, -2.1528, -2.1849],\n",
      "        [-2.1970, -2.2670, -2.1319,  ..., -1.1457, -2.2145, -2.3191],\n",
      "        ...,\n",
      "        [-2.2416, -2.2194, -2.1777,  ..., -2.1695, -2.1786, -2.1547],\n",
      "        [-2.1984, -2.2161, -2.1764,  ..., -2.1944, -2.1752, -2.1848],\n",
      "        [-2.1926, -2.2017, -2.1793,  ..., -2.2005, -2.1910, -2.1810]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2200, -2.2058, -2.1835,  ..., -2.1721, -2.1921, -2.1718],\n",
      "        [-2.2961, -2.2011, -2.1977,  ..., -1.1274, -2.1747, -2.2144],\n",
      "        [-2.1468, -2.2679, -2.2695,  ..., -1.1577, -2.2289, -2.2621],\n",
      "        ...,\n",
      "        [-2.2480, -2.1766, -2.1871,  ..., -2.1351, -2.1895, -2.2052],\n",
      "        [-2.1703, -2.2002, -2.1837,  ..., -2.2088, -2.2026, -2.1801],\n",
      "        [-2.1617, -2.1962, -2.2032,  ..., -2.2119, -2.1873, -2.1846]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2017, -2.2211, -2.1667,  ..., -2.1724, -2.1976, -2.1854],\n",
      "        [-2.1009, -2.1956, -2.1913,  ..., -1.1166, -2.1950, -2.2518],\n",
      "        [-2.0679, -2.2663, -2.2025,  ..., -1.1965, -2.2762, -2.3708],\n",
      "        ...,\n",
      "        [-2.1628, -2.1323, -2.2788,  ..., -2.1929, -2.2093, -2.1591],\n",
      "        [-2.1793, -2.2078, -2.2032,  ..., -2.1880, -2.1861, -2.1816],\n",
      "        [-2.1704, -2.1647, -2.2091,  ..., -2.1977, -2.2071, -2.1960]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2013, -2.1915, -2.1909,  ..., -2.1856, -2.1898, -2.1872],\n",
      "        [-2.2327, -2.2082, -2.2246,  ..., -1.1140, -2.1502, -2.1424],\n",
      "        [-2.2801, -2.2418, -2.2518,  ..., -1.1467, -2.2068, -2.1779],\n",
      "        ...,\n",
      "        [-2.1617, -2.1710, -2.2240,  ..., -2.2187, -2.1761, -2.1901],\n",
      "        [-2.1302, -2.1592, -2.2330,  ..., -2.2265, -2.1919, -2.1989],\n",
      "        [-2.1283, -2.1367, -2.2324,  ..., -2.2303, -2.2086, -2.2026]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2198, -2.1850, -2.1914,  ..., -2.1758, -2.1823, -2.1900],\n",
      "        [-2.1155, -2.1682, -2.1284,  ..., -1.1263, -2.1743, -2.1945],\n",
      "        [-2.3187, -2.1243, -2.1637,  ..., -1.1302, -2.1962, -2.2534],\n",
      "        ...,\n",
      "        [-2.1299, -2.0698, -2.3918,  ..., -2.2169, -2.1540, -2.1352],\n",
      "        [-2.1786, -2.1992, -2.2020,  ..., -2.1918, -2.1954, -2.1793],\n",
      "        [-2.1819, -2.1871, -2.2022,  ..., -2.1849, -2.2095, -2.1805]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1932, -2.1443, -2.1986,  ..., -2.2142, -2.2056, -2.1883],\n",
      "        [-2.1494, -2.2232, -2.2291,  ..., -1.1130, -2.1806, -2.1711],\n",
      "        [-2.2099, -2.2401, -2.3101,  ..., -1.1646, -2.2325, -2.1957],\n",
      "        ...,\n",
      "        [-2.1231, -2.2035, -2.2071,  ..., -2.2151, -2.1579, -2.2305],\n",
      "        [-2.1560, -2.1451, -2.2244,  ..., -2.2235, -2.1923, -2.2005],\n",
      "        [-2.1634, -2.1778, -2.1979,  ..., -2.2153, -2.1912, -2.1996]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1970, -2.1533, -2.2059,  ..., -2.1986, -2.1912, -2.1961],\n",
      "        [-2.1042, -2.2049, -2.2380,  ..., -1.1209, -2.1640, -2.2872],\n",
      "        [-2.1401, -2.2344, -2.2787,  ..., -1.1502, -2.2236, -2.2791],\n",
      "        ...,\n",
      "        [-2.2329, -2.2078, -2.1867,  ..., -2.1514, -2.1972, -2.1672],\n",
      "        [-2.1981, -2.1974, -2.1863,  ..., -2.1893, -2.1841, -2.1909],\n",
      "        [-2.1900, -2.2128, -2.1911,  ..., -2.1914, -2.1804, -2.1802]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1966, -2.1707, -2.2022,  ..., -2.1913, -2.1936, -2.1904],\n",
      "        [-2.1710, -2.2037, -2.3136,  ..., -1.1207, -2.1780, -2.1317],\n",
      "        [-2.0666, -2.3206, -2.3134,  ..., -1.1739, -2.2270, -2.2289],\n",
      "        ...,\n",
      "        [-2.1823, -2.1503, -2.2212,  ..., -2.1871, -2.1947, -2.2057],\n",
      "        [-2.1698, -2.1770, -2.2041,  ..., -2.2097, -2.1957, -2.1888],\n",
      "        [-2.1668, -2.2103, -2.1879,  ..., -2.2053, -2.1886, -2.1867]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 70: Train: 0.9766, Validation: 0.8817. Test: 0.8564, Loss: 1.1509\n",
      "tensor([[-2.1560, -2.2162, -2.1855,  ..., -2.2039, -2.1890, -2.1931],\n",
      "        [-2.2673, -2.1659, -2.2117,  ..., -1.1216, -2.1728, -2.0678],\n",
      "        [-2.3289, -2.2360, -2.1650,  ..., -1.2027, -2.2448, -2.2561],\n",
      "        ...,\n",
      "        [-2.2063, -2.2206, -2.1856,  ..., -2.2006, -2.2088, -2.1200],\n",
      "        [-2.2003, -2.2134, -2.1936,  ..., -2.1790, -2.1826, -2.1769],\n",
      "        [-2.1899, -2.1841, -2.1989,  ..., -2.1866, -2.2013, -2.1855]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2239, -2.1945, -2.1823,  ..., -2.1663, -2.1909, -2.1875],\n",
      "        [-2.1519, -2.1979, -2.1620,  ..., -1.1164, -2.2063, -2.2790],\n",
      "        [-2.0590, -2.2149, -2.1950,  ..., -1.1244, -2.2334, -2.2844],\n",
      "        ...,\n",
      "        [-2.2301, -2.1991, -2.1678,  ..., -2.1761, -2.1993, -2.1718],\n",
      "        [-2.1929, -2.2047, -2.1857,  ..., -2.1823, -2.1964, -2.1843],\n",
      "        [-2.1671, -2.2073, -2.1864,  ..., -2.2015, -2.1912, -2.1921]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2199, -2.2082, -2.1926,  ..., -2.1712, -2.1876, -2.1656],\n",
      "        [-2.2992, -2.1623, -2.0837,  ..., -1.1318, -2.2129, -2.2753],\n",
      "        [-2.1822, -2.2867, -2.1224,  ..., -1.1616, -2.2707, -2.2999],\n",
      "        ...,\n",
      "        [-2.2626, -2.1468, -2.2032,  ..., -2.1893, -2.2132, -2.1219],\n",
      "        [-2.1865, -2.2119, -2.1972,  ..., -2.1866, -2.1793, -2.1845],\n",
      "        [-2.1901, -2.2080, -2.1890,  ..., -2.1871, -2.1925, -2.1795]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2176, -2.2386, -2.1663,  ..., -2.1632, -2.1828, -2.1745],\n",
      "        [-2.2248, -2.2045, -2.2072,  ..., -1.1144, -2.1979, -2.1954],\n",
      "        [-2.1929, -2.2547, -2.2463,  ..., -1.1642, -2.2215, -2.2791],\n",
      "        ...,\n",
      "        [-2.1421, -2.1527, -2.2095,  ..., -2.2442, -2.2076, -2.1851],\n",
      "        [-2.1896, -2.2325, -2.1907,  ..., -2.1801, -2.1790, -2.1730],\n",
      "        [-2.1847, -2.2145, -2.1927,  ..., -2.1819, -2.1799, -2.1921]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1908, -2.1868, -2.2132,  ..., -2.1813, -2.1687, -2.2020],\n",
      "        [-2.3126, -2.1527, -2.2308,  ..., -1.1220, -2.1755, -2.1207],\n",
      "        [-2.2308, -2.2037, -2.2488,  ..., -1.1334, -2.2289, -2.2188],\n",
      "        ...,\n",
      "        [-2.1762, -2.1422, -2.2566,  ..., -2.1835, -2.1862, -2.1935],\n",
      "        [-2.1197, -2.1825, -2.2157,  ..., -2.2188, -2.1947, -2.2099],\n",
      "        [-2.1095, -2.1863, -2.2259,  ..., -2.2276, -2.2030, -2.1875]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1807, -2.2282, -2.1861,  ..., -2.1857, -2.1845, -2.1798],\n",
      "        [-2.2401, -2.1708, -2.2386,  ..., -1.1145, -2.1628, -2.1423],\n",
      "        [-2.2150, -2.2344, -2.2436,  ..., -1.1326, -2.2142, -2.2220],\n",
      "        ...,\n",
      "        [-2.1685, -2.2001, -2.2062,  ..., -2.1265, -2.2018, -2.2382],\n",
      "        [-2.1701, -2.1633, -2.2225,  ..., -2.1978, -2.1854, -2.2052],\n",
      "        [-2.1599, -2.1798, -2.2161,  ..., -2.2043, -2.1880, -2.1968]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2219, -2.1994, -2.1742,  ..., -2.1752, -2.1982, -2.1744],\n",
      "        [-2.2354, -2.1975, -2.1539,  ..., -1.1238, -2.1027, -2.3019],\n",
      "        [-2.2721, -2.2263, -2.1596,  ..., -1.1354, -2.2288, -2.2371],\n",
      "        ...,\n",
      "        [-2.0646, -2.0203, -2.3070,  ..., -2.2677, -2.1962, -2.2399],\n",
      "        [-2.1939, -2.2127, -2.1909,  ..., -2.1921, -2.1741, -2.1820],\n",
      "        [-2.1905, -2.2212, -2.1856,  ..., -2.1894, -2.1765, -2.1823]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1889, -2.1916, -2.2025,  ..., -2.1990, -2.1855, -2.1786],\n",
      "        [-2.2395, -2.1953, -2.1588,  ..., -1.1129, -2.1916, -2.2002],\n",
      "        [-2.2078, -2.2477, -2.2414,  ..., -1.1333, -2.2104, -2.2220],\n",
      "        ...,\n",
      "        [-2.1876, -2.2064, -2.1896,  ..., -2.2078, -2.1390, -2.2134],\n",
      "        [-2.1172, -2.1843, -2.2085,  ..., -2.2327, -2.1905, -2.2073],\n",
      "        [-2.1173, -2.2001, -2.2088,  ..., -2.2310, -2.1915, -2.1923]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1941, -2.2033, -2.1843,  ..., -2.1927, -2.1847, -2.1867],\n",
      "        [-2.1378, -2.2147, -2.1752,  ..., -1.1147, -2.1962, -2.2541],\n",
      "        [-2.1218, -2.2751, -2.1894,  ..., -1.1677, -2.2502, -2.3336],\n",
      "        ...,\n",
      "        [-2.1113, -2.1589, -2.2250,  ..., -2.2243, -2.1791, -2.2354],\n",
      "        [-2.1712, -2.2074, -2.1862,  ..., -2.2047, -2.1864, -2.1900],\n",
      "        [-2.1503, -2.1863, -2.1905,  ..., -2.2218, -2.2038, -2.1914]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1947, -2.2044, -2.1849,  ..., -2.1906, -2.1877, -2.1841],\n",
      "        [-2.2136, -2.2033, -2.1927,  ..., -1.1132, -2.1613, -2.2285],\n",
      "        [-2.1578, -2.2151, -2.3062,  ..., -1.1357, -2.1969, -2.2394],\n",
      "        ...,\n",
      "        [-2.0356, -2.1716, -2.2258,  ..., -2.2446, -2.2075, -2.2391],\n",
      "        [-2.1300, -2.2037, -2.1977,  ..., -2.2243, -2.1929, -2.1942],\n",
      "        [-2.1415, -2.2195, -2.1926,  ..., -2.2136, -2.1892, -2.1874]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 80: Train: 0.9809, Validation: 0.8780. Test: 0.8600, Loss: 1.1495\n",
      "tensor([[-2.2029, -2.1965, -2.1810,  ..., -2.1884, -2.1907, -2.1866],\n",
      "        [-2.2410, -2.2031, -2.1902,  ..., -1.1178, -2.1746, -2.2333],\n",
      "        [-2.1403, -2.2536, -2.3183,  ..., -1.1663, -2.1911, -2.2747],\n",
      "        ...,\n",
      "        [-2.1817, -2.2164, -2.1880,  ..., -2.1951, -2.1624, -2.2018],\n",
      "        [-2.1321, -2.1762, -2.2032,  ..., -2.2278, -2.1989, -2.2044],\n",
      "        [-2.1322, -2.1803, -2.2066,  ..., -2.2268, -2.1927, -2.2037]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1970, -2.1820, -2.1913,  ..., -2.2035, -2.1838, -2.1883],\n",
      "        [-2.1953, -2.2166, -2.1207,  ..., -1.1180, -2.1794, -2.2798],\n",
      "        [-2.2547, -2.2151, -2.1813,  ..., -1.1495, -2.2269, -2.2846],\n",
      "        ...,\n",
      "        [-2.1986, -2.1949, -2.2071,  ..., -2.1813, -2.1840, -2.1802],\n",
      "        [-2.1545, -2.1864, -2.2020,  ..., -2.2058, -2.1944, -2.2019],\n",
      "        [-2.1440, -2.1878, -2.2019,  ..., -2.2191, -2.1909, -2.2006]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2237, -2.1836, -2.1781,  ..., -2.1840, -2.1960, -2.1798],\n",
      "        [-2.1351, -2.1574, -2.2638,  ..., -1.1257, -2.1813, -2.0816],\n",
      "        [-2.3153, -2.2070, -2.2109,  ..., -1.1594, -2.2625, -2.1817],\n",
      "        ...,\n",
      "        [-2.2414, -2.2242, -2.2187,  ..., -2.2153, -1.9606, -2.2430],\n",
      "        [-2.1752, -2.1948, -2.1910,  ..., -2.2042, -2.1918, -2.1891],\n",
      "        [-2.1685, -2.1896, -2.1923,  ..., -2.2134, -2.1959, -2.1860]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2255, -2.1950, -2.1625,  ..., -2.1851, -2.2012, -2.1744],\n",
      "        [-2.2422, -2.1738, -2.1285,  ..., -1.1169, -2.2200, -2.1293],\n",
      "        [-2.2040, -2.2092, -2.2001,  ..., -1.1354, -2.2352, -2.2790],\n",
      "        ...,\n",
      "        [-2.1438, -2.2104, -2.1537,  ..., -2.2370, -2.2060, -2.1908],\n",
      "        [-2.1502, -2.1734, -2.2009,  ..., -2.2174, -2.2102, -2.1920],\n",
      "        [-2.1447, -2.1849, -2.1947,  ..., -2.2237, -2.2056, -2.1904]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1782, -2.1921, -2.1835,  ..., -2.2075, -2.1959, -2.1890],\n",
      "        [-2.2236, -2.1917, -2.1694,  ..., -1.1123, -2.2180, -2.1738],\n",
      "        [-2.1286, -2.1898, -2.2218,  ..., -1.1223, -2.2358, -2.2718],\n",
      "        ...,\n",
      "        [-2.2159, -2.1722, -2.1695,  ..., -2.2290, -2.2037, -2.1529],\n",
      "        [-2.1269, -2.1907, -2.2118,  ..., -2.2352, -2.1813, -2.1957],\n",
      "        [-2.1313, -2.1742, -2.2022,  ..., -2.2301, -2.2074, -2.1971]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1993, -2.1833, -2.1766,  ..., -2.2070, -2.2021, -2.1774],\n",
      "        [-2.1714, -2.1886, -2.2294,  ..., -1.1120, -2.1941, -2.1869],\n",
      "        [-2.1678, -2.2558, -2.2431,  ..., -1.1354, -2.2293, -2.2344],\n",
      "        ...,\n",
      "        [-2.2811, -2.2072, -2.2015,  ..., -2.1695, -2.2123, -2.0542],\n",
      "        [-2.1830, -2.1890, -2.1882,  ..., -2.2074, -2.1952, -2.1835],\n",
      "        [-2.1924, -2.1890, -2.1809,  ..., -2.2031, -2.1946, -2.1863]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2343, -2.1983, -2.1802,  ..., -2.1734, -2.1803, -2.1780],\n",
      "        [-2.1592, -2.2077, -2.2236,  ..., -1.1122, -2.1969, -2.1758],\n",
      "        [-2.1803, -2.2374, -2.1836,  ..., -1.1179, -2.2127, -2.2361],\n",
      "        ...,\n",
      "        [-2.2213, -2.1715, -2.1851,  ..., -2.2060, -2.1935, -2.1676],\n",
      "        [-2.1921, -2.1961, -2.1931,  ..., -2.1980, -2.1696, -2.1969],\n",
      "        [-2.1832, -2.1872, -2.1908,  ..., -2.1986, -2.1938, -2.1928]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2274, -2.1977, -2.1815,  ..., -2.1706, -2.1801, -2.1874],\n",
      "        [-2.2025, -2.1968, -2.1898,  ..., -1.1130, -2.1911, -2.2269],\n",
      "        [-2.1729, -2.2207, -2.2210,  ..., -1.1260, -2.2204, -2.2614],\n",
      "        ...,\n",
      "        [-2.2372, -2.1892, -2.1865,  ..., -2.1689, -2.1699, -2.1926],\n",
      "        [-2.2244, -2.2047, -2.1804,  ..., -2.1776, -2.1712, -2.1869],\n",
      "        [-2.2177, -2.1896, -2.1906,  ..., -2.1791, -2.1777, -2.1912]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2162, -2.2073, -2.1882,  ..., -2.1734, -2.1701, -2.1901],\n",
      "        [-2.1598, -2.2126, -2.2486,  ..., -1.1136, -2.1604, -2.1814],\n",
      "        [-2.2820, -2.1986, -2.2500,  ..., -1.1400, -2.2162, -2.1953],\n",
      "        ...,\n",
      "        [-2.2370, -2.1961, -2.2008,  ..., -2.1649, -2.1562, -2.1883],\n",
      "        [-2.1938, -2.1986, -2.1937,  ..., -2.1901, -2.1756, -2.1945],\n",
      "        [-2.1856, -2.1885, -2.2003,  ..., -2.1941, -2.1755, -2.2021]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2240, -2.1670, -2.1908,  ..., -2.1861, -2.1835, -2.1938],\n",
      "        [-2.2012, -2.1841, -2.2184,  ..., -1.1122, -2.1650, -2.2072],\n",
      "        [-2.1325, -2.2265, -2.2710,  ..., -1.1216, -2.1895, -2.2282],\n",
      "        ...,\n",
      "        [-2.2193, -2.1824, -2.1858,  ..., -2.1956, -2.1829, -2.1795],\n",
      "        [-2.2006, -2.1894, -2.1900,  ..., -2.1929, -2.1826, -2.1909],\n",
      "        [-2.2023, -2.1991, -2.1874,  ..., -2.1885, -2.1787, -2.1904]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 90: Train: 0.9821, Validation: 0.8762. Test: 0.8545, Loss: 1.1432\n",
      "tensor([[-2.1981, -2.1808, -2.1943,  ..., -2.1948, -2.1886, -2.1899],\n",
      "        [-2.2728, -2.1678, -2.1754,  ..., -1.1164, -2.2262, -2.1502],\n",
      "        [-2.1791, -2.2258, -2.1859,  ..., -1.1185, -2.2406, -2.2154],\n",
      "        ...,\n",
      "        [-2.2331, -2.1892, -2.1891,  ..., -2.2144, -2.1860, -2.1303],\n",
      "        [-2.1784, -2.1973, -2.1932,  ..., -2.2010, -2.1808, -2.1954],\n",
      "        [-2.1800, -2.1960, -2.1942,  ..., -2.2019, -2.1849, -2.1893]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2161, -2.1865, -2.1834,  ..., -2.1874, -2.1883, -2.1842],\n",
      "        [-2.2797, -2.1512, -2.1353,  ..., -1.1169, -2.2064, -2.1567],\n",
      "        [-2.0617, -2.2136, -2.3085,  ..., -1.1514, -2.2015, -2.3187],\n",
      "        ...,\n",
      "        [-2.2427, -2.1787, -2.1852,  ..., -2.2099, -2.1869, -2.1388],\n",
      "        [-2.1984, -2.1854, -2.1914,  ..., -2.1980, -2.1803, -2.1929],\n",
      "        [-2.2062, -2.1955, -2.1784,  ..., -2.1968, -2.1793, -2.1899]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2196, -2.1886, -2.1715,  ..., -2.1768, -2.1952, -2.1932],\n",
      "        [-2.2225, -2.1919, -2.2046,  ..., -1.1134, -2.1636, -2.2207],\n",
      "        [-2.1816, -2.2018, -2.2444,  ..., -1.1206, -2.2105, -2.2325],\n",
      "        ...,\n",
      "        [-2.2404, -2.1885, -2.1853,  ..., -2.1852, -2.1844, -2.1603],\n",
      "        [-2.1879, -2.1901, -2.1892,  ..., -2.1915, -2.1924, -2.1955],\n",
      "        [-2.1853, -2.1894, -2.1866,  ..., -2.1932, -2.1954, -2.1966]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.2071, -2.1806, -2.1860,  ..., -2.1719, -2.1928, -2.2074],\n",
      "        [-2.1335, -2.2040, -2.2368,  ..., -1.1138, -2.1624, -2.2063],\n",
      "        [-2.1651, -2.2314, -2.2876,  ..., -1.1311, -2.1619, -2.2498],\n",
      "        ...,\n",
      "        [-2.2136, -2.2027, -2.1867,  ..., -2.1554, -2.1780, -2.2085],\n",
      "        [-2.1483, -2.1828, -2.1962,  ..., -2.2034, -2.1984, -2.2154],\n",
      "        [-2.1560, -2.1847, -2.1880,  ..., -2.2028, -2.2008, -2.2127]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1922, -2.2041, -2.1889,  ..., -2.1646, -2.1863, -2.2095],\n",
      "        [-2.1190, -2.2013, -2.2288,  ..., -1.1144, -2.1887, -2.2256],\n",
      "        [-2.1148, -2.2230, -2.3351,  ..., -1.1585, -2.2284, -2.2532],\n",
      "        ...,\n",
      "        [-2.2018, -2.1876, -2.1697,  ..., -2.1507, -2.1952, -2.2380],\n",
      "        [-2.1654, -2.2015, -2.1934,  ..., -2.1905, -2.1807, -2.2141],\n",
      "        [-2.1610, -2.1944, -2.1928,  ..., -2.1904, -2.1944, -2.2126]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1874, -2.1836, -2.1843,  ..., -2.1766, -2.2042, -2.2094],\n",
      "        [-2.1824, -2.1685, -2.1922,  ..., -1.1115, -2.1879, -2.2055],\n",
      "        [-2.0677, -2.2103, -2.2391,  ..., -1.1208, -2.2076, -2.2480],\n",
      "        ...,\n",
      "        [-2.1966, -2.1945, -2.2007,  ..., -2.1513, -2.1980, -2.2037],\n",
      "        [-2.1455, -2.1762, -2.2051,  ..., -2.1979, -2.1965, -2.2227],\n",
      "        [-2.1445, -2.1979, -2.1988,  ..., -2.1972, -2.1936, -2.2122]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1535, -2.1809, -2.1934,  ..., -2.2007, -2.2125, -2.2040],\n",
      "        [-2.2156, -2.1608, -2.2097,  ..., -1.1125, -2.1923, -2.2068],\n",
      "        [-2.2216, -2.1882, -2.2320,  ..., -1.1187, -2.2288, -2.1909],\n",
      "        ...,\n",
      "        [-2.1933, -2.1502, -2.2019,  ..., -2.1541, -2.2189, -2.2245],\n",
      "        [-2.1673, -2.1834, -2.1981,  ..., -2.1963, -2.2000, -2.2007],\n",
      "        [-2.1629, -2.1966, -2.1912,  ..., -2.1996, -2.1973, -2.1981]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1873, -2.2030, -2.1825,  ..., -2.1907, -2.2027, -2.1798],\n",
      "        [-2.1733, -2.1898, -2.1896,  ..., -1.1112, -2.1908, -2.2049],\n",
      "        [-2.1989, -2.2134, -2.1765,  ..., -1.1178, -2.2256, -2.2388],\n",
      "        ...,\n",
      "        [-2.2004, -2.1612, -2.2018,  ..., -2.1747, -2.2207, -2.1860],\n",
      "        [-2.1679, -2.1938, -2.1935,  ..., -2.2072, -2.1963, -2.1873],\n",
      "        [-2.1626, -2.1852, -2.1961,  ..., -2.2077, -2.2015, -2.1925]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1906, -2.2067, -2.1800,  ..., -2.2025, -2.1960, -2.1698],\n",
      "        [-2.1795, -2.1834, -2.2342,  ..., -1.1172, -2.1829, -2.1003],\n",
      "        [-2.1770, -2.2274, -2.2182,  ..., -1.1182, -2.2477, -2.1704],\n",
      "        ...,\n",
      "        [-2.1905, -2.1916, -2.1842,  ..., -2.1889, -2.2039, -2.1870],\n",
      "        [-2.1745, -2.1981, -2.1822,  ..., -2.2180, -2.1930, -2.1798],\n",
      "        [-2.1630, -2.1857, -2.1890,  ..., -2.2188, -2.2050, -2.1837]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "tensor([[-2.1974, -2.2089, -2.1750,  ..., -2.2108, -2.1861, -2.1669],\n",
      "        [-2.2112, -2.1921, -2.1961,  ..., -1.1119, -2.1853, -2.1547],\n",
      "        [-2.0796, -2.2527, -2.3246,  ..., -1.1357, -2.2037, -2.2052],\n",
      "        ...,\n",
      "        [-2.2077, -2.1873, -2.1722,  ..., -2.2287, -2.2066, -2.1401],\n",
      "        [-2.1882, -2.2058, -2.1854,  ..., -2.2208, -2.1738, -2.1707],\n",
      "        [-2.1836, -2.1965, -2.1839,  ..., -2.2179, -2.1958, -2.1675]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "loss_grad:None\n",
      "Epoch 100: Train: 0.9846, Validation: 0.8725. Test: 0.8545, Loss: 1.1397\n",
      "GNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): ourGNNLayer(1433, 32)\n",
      "    (1): ourGNNLayer(32, 32)\n",
      "    (2): ourGNNLayer(32, 7)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 开始训练\n",
    "model, train_acc, val_acc, test_acc = train(train_loader, val_loader,test_loader,args, num_node_features, num_classes, args[\"device\"])\n",
    "\n",
    "# 打印模型\n",
    "print(model)\n",
    "\n",
    "# 可视化训练过程\n",
    "plt.title(pyg_dataset.name + \" (\"+ args['model'] + \")\")\n",
    "plt.plot(train_acc, label=\"training accuracy\")\n",
    "plt.plot(val_acc, label=\"val accuracy\")\n",
    "plt.plot(test_acc, label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}